# -*- coding: utf-8 -*-
"""ProcessingDatasetSpotify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OxmTz5LqqG5oEH0IYyjvbGCzoS98mcOw
"""

!pip install pydub

!pip install pydub=0.25.1-py2.py3-none-any.whl

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os
import time
from pydub import AudioSegment
import shutil
import re

"""# Pre Processing Dataset

Transcription
1. Finds all "JSON" files
2. Make all files into a dataframe
3. Remove unnecessary columns
4. Create a column named `timestamp` [end_time - start_time]
5. Every 10 seconds create a group.
6. Aggregate rows by group -> sentences, start_time and end_time
7. Save to csv file

Classe PreprocessingDataset

1. percorre os diretórios encontra arquivos .json para realizar a transcrição completa dos áudios</br>
2. converte arquivos .ogg em wav</br>
3. armazena informações em um arquivo</br>

Output:

Arquivo .csv - metadados_train_full.csv |</br>
"""

class PreprocessingDataset():

    def __init__(self,directory=None,
                 save_directory=None,
                 out_Path=None,
                 time_seconds=None):
        self.directory = directory
        self.save_directory = save_directory
        self.out_path = out_Path
        self.time_seconds = float(time_seconds) 

    
    def copy__other_path(self):

        for dirpath, dirnames, filenames in os.walk(self.directory):
             for i,filename in enumerate (filenames):
                if filename.endswith(".ogg"):
                    src = os.path.join(dirpath , filename)
                    dest = os.path.join(self.save_directory, filename)
                    shutil.copy2(src, dest)
    

    def read_json_file(self,path):
        df = pd.read_json(path)
        print(df.columns)
        columns = ['audio_file_id','vendor','duration','transcription_date','transcript_text' ]
        #columns = ['vendor','duration','transcription_date','transcript_text' ]
        df.drop(columns, inplace=True, axis=1)
        df = df['words'].apply(pd.Series)
        return df


    def generate_complete_sequence(self,path,filename):

        df = self.read_json_file(path)
        df['timestamp'] = df['end_time_secs'] - df['start_time_secs']
        sum = 0
        group = 1
        df['group'] = 0
        
        '''
            Aggregate words -> segments
            for each time seconds
        '''

        for index, row in df.iterrows():
            sum += row['timestamp']
            if sum > 10.0:
                sum = row['timestamp']
                group += 1
            df.at[index, 'group'] = group
        df = df.groupby('group', as_index=False).agg({'word' : ' '.join, 'start_time_secs' : 'first', 'end_time_secs':'last'})
        df['name_episode'] = filename
        
        episodes = df.groupby('name_episode').apply(
            lambda x: x.loc[:, ['start_time_secs', 'end_time_secs']].to_dict('records')
        )
        print(episodes)
        self.trim_multiple_audios(episodes)

    def merge_all_files(self):

        files_csv_list = [self.out_path + f for f in os.listdir(self.out_path)]

        csv_list = []

        for file in sorted(files_csv_list):
            csv_list.append(pd.read_csv(file).assign(File_Name = os.path.basename(file)))

        csv_merged = pd.concat(csv_list, ignore_index=True)
        csv_merged.to_csv(self.out_path + 'metadados_train_full.csv', index=False)

    def trim_multiple_audios(self,episodes):

        path_audio_files = "/content/drive/MyDrive/USP/qualificacao/pasta_/"
        for episode_id, sections in episodes.items():
            audio = AudioSegment.from_file(path_audio_files + f"{episode_id}.ogg", format="ogg")
            for i, section in enumerate(sections):
             
                start = int(section["start_time_secs"] * 1000) #segundos -> milisegundos
                finish = int(section["end_time_secs"] * 1000)
                segment = audio[start:finish]
                directory_ = "/content/drive/MyDrive/USP/qualificacao/spotify_transcription/"
                #if not os.path.isdir(directory):
                #os.makedirs(directory)
                segment.export(directory_ + f"{episode_id}_segment{i}.wav", format="wav")
                #else:
                #segment.export(directory + f"{episode_id}_segment{i}.wav", format="wav")

    def main(self):

        for dirpath, dirnames, filenames in os.walk(self.directory):
            for i, filename in enumerate(filenames):
                if filename.endswith('.json'):
                    result = re.search('locais/(.*)/show', dirpath)
                    accent = (result.group(1))
                    src = os.path.join(dirpath,filename)
                    self.generate_complete_sequence(src,filename.replace('.json',''))

start_time = time.time()

directory= '/content/drive/MyDrive/USP/qualificacao/teste/'
save_directory = '/content/drive/MyDrive/USP/qualificacao/pasta_/'
out_path = '/content/drive/MyDrive/USP/qualificacao/pasta_/'


preprocessing = PreprocessingDataset(directory,save_directory,out_path,10.0)
preprocessing.main() 
#preprocessing.merge_all_files(out_path)

print("--- %s seconds ---" % (time.time() - start_time))

"""# Teste"""

def trim_audio_single_test(filename):
        df = generate_complete_sequence()
        start_time_secs_list = df['start_time_secs'].tolist()
        end_time_secs_list = df['end_time_secs'].tolist()
        audio = AudioSegment.from_file(filename)
        index = 0
        for l, n in zip(start_time_secs_list,end_time_secs_list):
              start_time = l * 1000
              end_time =  n * 1000
              index+=1
              cropped_audio = audio[start_time:end_time]
              cropped_audio_filename = '/content/drive/MyDrive/USP/teste_audio/'+filename + str(index)
              cropped_audio.export(cropped_audio_filename, format="ogg")

"""# Pré-Processamento Áudio

<h3>Separar música da voz</h3>

*  [**spleeter**](https://github.com/deezer/spleeter)
*  [repet](https://github.com/zafarrafii/REPET-Python)
*  [PodcastMix](https://github.com/MTG/Podcastmix)

<h3>Remover volume abaixo do threshold</h3>


*   Ao separar a voz da música, tem partes do áudio que ficaram quase inaudíveis, portanto foi necessário remover essas partes com um volume abaixo do threshold indicado
"""

!pip install ffmpeg-python

!pip install spleeter

!pip install tqdm

import os
import numpy as np
import pandas as pd
import librosa
import soundfile as sf
import multiprocessing as mp
from tqdm import tqdm
from spleeter.separator import Separator

destination_path = "/content/drive/MyDrive/USP/qualificacao/dataset_teste/SP/"
for dirpath, dirnames, filenames in os.walk(destination_path):
    for i, filename in enumerate(filenames):
        audio_file = os.path.join(dirpath, filename)
        #print(filename)
        i+=1
print(i)

"""# Separar voz da música"""

from pathlib import Path

path_with_folders = "/content/drive/MyDrive/USP/qualificacao/dataset_teste/BA"
batch_size = 20
destination_path = "/content/drive/MyDrive/USP/qualificacao/dataset_teste/teste_arquivos/"

class CleaningAudiosDataset(object):

    def __init__(self,path_with_folders,save_local_folder,batch_size,destination_path):
        self.path_with_folders = path_with_folders
        self.save_local_folder = save_local_folder
        self.batch_size = batch_size
        self.destination_path = destination_path

    def separate_voice_from_music(self):
        
        audio_files = [os.path.join(self.path_with_folders, f) for f in os.listdir(self.path_with_folders) if f.endswith('.wav')]

        batches = [audio_files[i:i + self.batch_size] for i in range(0, len(audio_files), self.batch_size)]
        separator = Separator('spleeter:2stems') 
        
        for batch in tqdm(batches):
            for audio_file in batch:
                prediction = separator.separate_to_file(audio_file, self.save_local_folder)

    def rename_and_copy(self,path):
        for dirpath,dirname,filenames in os.walk(path):
            for i, filename in enumerate(filenames):
                audio_file = os.path.join(dirpath,filename)
                if  not (audio_file.endswith('accompaniment.wav')):
                    dir_name = os.path.basename(dirpath)
                    file_name = os.path.basename(filename)
                    new_file = os.path.join(dirpath,f"{dir_name}.wav")
                    #os.rename(audio_file, new_file)
                    new_path = os.path.join(self.destination_path, f"{dir_name}.wav")
                    shutil.move(audio_file, new_path)


    def remove_low_volume(self,threshold):
        for file in os.listdir(self.destination_path):
            if file.endswith('.wav'):
                sound = AudioSegment.from_file(os.path.join(self.destination_path,file),format='wav')
                loud = [seg for seg in sound if seg.dBFS > threshold]
                if loud: 
                    new = loud[0]
                    for l in loud[1:]:
                        new+=l

                    output = os.path.join(self.destination_path,file)
                    new.export(output,format='wav')
                else:
                    print("Low loud")
                

    def main(self):

        self.separate_voice_from_music()

clean_audios = CleaningAudiosDataset(path_with_folders,
                      batch_size,
                      destination_path)
clean_audios.separate_voice_from_music()

path_with_folders = "/content/drive/MyDrive/USP/qualificacao/dataset_teste/PE/"
save_local_folder = "/content/drive/MyDrive/USP/qualificacao/teste/"
batch_size = 20
destination_path = "/content/drive/MyDrive/USP/qualificacao/teste/"



clean_audios = CleaningAudiosDataset(path_with_folders,
                      save_local_folder,
                      batch_size,
                      destination_path)
clean_audios.main()

!pip install pyAudioAnalysis

from pydub import AudioSegment

sound = AudioSegment.from_file("/content/6OMFXfrATEsgpql8AwbwYN_1.wav", format="wav")
volume_thresh = -50.0
loud_parts = [part for part in sound if part.dBFS > volume_thresh]
new_sound = loud_parts[0]
for part in loud_parts[1:]:
    new_sound += part

new_sound.export("output56.wav", format="wav")



"""# Diarizador

diarizador (separa falar por locutor)

---

verificador de locutor / identificador de locutor (diz quem é determinado locutor)

--------------------------------

whisper
"""

!pip install -q https://github.com/pyannote/pyannote-audio/tarball/develop

!wget -q http://groups.inf.ed.ac.uk/ami/AMICorpusMirror/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav
DEMO_FILE = {'uri': 'ES2004a.Mix-Headset', 'audio': 'ES2004a.Mix-Headset.wav'}

!wget -q https://raw.githubusercontent.com/pyannote/pyannote-audio/develop/tutorials/data_preparation/AMI/MixHeadset.test.rttm

pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null

!pip install -q git+https://github.com/openai/whisper.git > /dev/null

import librosa
import numpy as np
from sklearn.cluster import KMeans
import os

!pip uninstall torchvision -y
!pip uninstall torch -y
!pip install torch torchvision

import whisper
import datetime

import subprocess

import torch
import pyannote.audio
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
embedding_model = PretrainedSpeakerEmbedding( 
    "speechbrain/spkrec-ecapa-voxceleb",
    device=torch.device("cuda"))

from pyannote.audio import Audio
from pyannote.core import Segment

import wave
import contextlib

from sklearn.cluster import AgglomerativeClustering
import numpy as np

dir = '/content/drive/MyDrive/teste/teste/'

num_speakers = 2 #@param {type:"integer"}

language = 'any' #@param ['any', 'English']

model_size = 'medium' #@param ['tiny', 'base', 'small', 'medium', 'large']


model_name = model_size
if language == 'English' and model_size != 'large':
  model_name += '.en'

model = whisper.load_model(model_size)

from abc import ABC

class DiarizationAudioFiles(object):

    def __init__(self,dir):
        self.dir = dir
        self.audio = Audio()
   
    def duration_context_audio(self,directory):
        with contextlib.closing(wave.open(directory,'r')) as f:
            frames = f.getnframes()
            rate = f.getframerate()
            duration = frames / float(rate)
        return duration

    def transcribe_segments_files(self):

        list_duration_audios = []
        list_segments=[]
        list_audios = []
        for audios in os.listdir(self.dir):
            audio_file = os.path.join(self.dir,audios)
            duration = self.duration_context_audio(audio_file)
            result = model.transcribe(audio_file)
            segments = result['segments']
            print(segments)
            list_audios.append(audio_file)
            list_duration_audios.append(duration)
            list_segments.append(segments)

        return list_duration_audios,list_segments,list_audios
   
    def segment_embedding(self,segment,duration,audio_file):
    
        start = segment['start']
        end = min(duration,segment['end'])
        clip = Segment(start,end)
        waveform,sample_rate = self.audio.crop(audio_file,clip)
        return embedding_model(waveform[None])


    def add_speaker_name(self):
        embeddings = self.get_embeddings()
        _,list_segments, list_audios = self.transcribe_segments_files()
        num_speakers = 2
        clustering = AgglomerativeClustering(num_speakers).fit(embeddings)
        labels = clustering.labels_
        for segments in list_segments:
            for i,segment in enumerate(segments):
                segment['speaker'] = 'SPEAKER' + str(labels[i] + 1)
        return list_segments

    def transcript_by_speaker_identification(self):
        list_segments = self.add_speaker_name()
        file =  open("arquivo.txt","w+",encoding='utf-8')
        for segments in list_segments:
            for i,segment in enumerate(segments):
                if i == 0 or segments[i-1]['speaker'] != segment['speaker']:
                    file.write('\n'+ segment["speaker"] + ' ' + time.strftime('%H:%M:%S', time.gmtime(segment["start"])) + '\n')
                file.write(segment["text"][1:] + ' ')
        file.close()

    def get_embeddings(self):
        list_duration_audios,list_segments, list_audios = self.transcribe_segments_files()
        embeddings = np.zeros(shape=(len(list_segments), 192))
        for i,(segment, duration, audio_file) in enumerate(zip(list_segments, list_duration_audios, list_audios)):
            embeddings[i] = diarization_audio.segment_embedding(segment[i],duration,audio_file)
        embeddings = np.nan_to_num(embeddings)
        return embeddings

diarization_audio = DiarizationAudioFiles(dir)

diarization_audio.transcript_by_speaker_identification()

list_duration_audios,list_segments, list_audios = diarization_audio.transcribe_segments_files()

len(list_segments)

num_speakers = 2
clustering = AgglomerativeClustering(num_speakers).fit(embeddings)
labels = clustering.labels_
for segments in list_segments:
    for i,segment in enumerate(segments):
        segment['speaker'] = 'SPEAKER' + str(labels[i] + 1)

list_segments

li

segments

clustering = AgglomerativeClustering(num_speakers).fit(embeddings)
labels = clustering.labels_
for i in enumerate(segments)):
  segments[i]["speaker"] = 'SPEAKER ' + str(labels[i] + 1)

def time(secs):
  return datetime.timedelta(seconds=round(secs))

f = open("transcript1.txt", "w",encoding='utf-8')

for (i, segment) in enumerate(segments):
  if i == 0 or segments[i - 1]["speaker"] != segment["speaker"]:
    f.write("\n" + segment["speaker"] + ' ' + str(time(segment["start"])) + '\n')
  f.write(segment["text"][1:] + ' ')
f.close()

len(segments)

segments





for line in os.listdir(dir):
    l = os.path.join(dir,line)
    print(l)
    
 
    y,sr = librosa.load(l)
    y = librosa.effects.trim(y)[0]
    y = librosa.effects.preemphasis(y)
    y = librosa.util.normalize(y)

    mfcc = librosa.feature.mfcc(y=y,sr=sr,n_mfcc=13)
    kmeans = KMeans(n_clusters=4, random_state=0).fit(mfcc.T)
    print(kmeans.labels_)

from pydub import AudioSegment

t1 = 0 * 1000 # works in milliseconds
t2 = 20 * 60 * 1000

newAudio = AudioSegment.from_wav("/content/4xjJY6vcBhBKYmb8H2cBhT_4.wav")
a = newAudio[t1:t2]
a.export("audio.wav", format="wav")

!pip install pyannote.audio

from pyannote.audio import Pipeline

pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')

DEMO_FILE = {'uri': 'blabal', 'audio': 'audio.wav'}
dz = pipeline(DEMO_FILE)  

with open("/content/drive/MyDrive/USP/qualificacao/diarization.txt", "w") as text_file:
    text_file.write(str(dz))